<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: akka-http | Zuchos.com]]></title>
  <link href="http://zuchos.github.io/blog/categories/akka-http/atom.xml" rel="self"/>
  <link href="http://zuchos.github.io/"/>
  <updated>2015-06-10T12:17:07+02:00</updated>
  <id>http://zuchos.github.io/</id>
  <author>
    <name><![CDATA[Łukasz Żuchowski]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[More reactive Publisher (aka Publisher vol. 2)]]></title>
    <link href="http://zuchos.github.io/blog/2015/06/10/more-reactive-publisher-aka-publisher-vol-2/"/>
    <updated>2015-06-10T15:06:51+02:00</updated>
    <id>http://zuchos.github.io/blog/2015/06/10/more-reactive-publisher-aka-publisher-vol-2</id>
    <content type="html"><![CDATA[<p>In my previous post I created <code>Publisher</code> for akka-streams that was buffering incoming data and then passing those down the stream. Johannes Rudolph aptly observed that the flow of that solution is the buffer overflow scenario (too many incoming requests may lead to out-of-memory issue).</p>

<blockquote><p>Thanks for the post! It&rsquo;s nice to see that people are actually starting to use akka-stream and akka-http. A note: implementing <code>ActorPublisher</code> shouldn&rsquo;t be necessary in most cases. In this case you built an unbounded buffer in front of a stream which defeats akka-stream/reactive streams back pressure logic. Now if the consumer cannot keep up with reading the data all the unwritten data will start to pile up in memory. Generally, it isn&rsquo;t possible to switch from a pull-style (akka-stream/reactive-stream) model to a push-style model (actor message tell) somewhere in the processing chain. In cases where you still need to do this (e.g. because you are dealing with a &ldquo;live&rdquo; data source) there&rsquo;s a somewhat safer option: use <a href="https://github.com/akka/akka/blob/release-2.3-dev/akka-stream/src/main/scala/akka/stream/scaladsl/Source.scala#L342"><code>Source.actorRef</code></a> which lets you define a limited buffer and makes you choose a strategy what to do when the buffer is full. <cite><a href="https://twitter.com/virtualvoid">Johannes Rudolph</a></cite></p></blockquote>

<p>First I would like to explain my motivation, this case comes from my pet project. In that project I&rsquo;m expecting that users around the world will send me the data, so I want to make API as simple as possible (what could be simpler than REST API?).
Users are not interested in the result of computation, they are interested in contributing the data. So the system should accept as many data as possible (return status 202 - Accepted to the user - it would mean that we received the data) and the process it with it&rsquo;s own speed. I rather expect to have many request from different user than one user will be sending tons of those.
The buffer overflow is possible situation here so Johannes was right that it should be tackled. First I took a look on proposed solution <code>Source.actorRef()</code>. The problem with <code>ActorRefSourceActor</code> is that the all available <code>OverflowStrategy</code> values are not notifying sender that the problem occurred and that leads to lost of data. So I couldn&rsquo;t use that solution.</p>

<p>So I came up with different one, I added bufferSize <code>val</code> to <code>DataPublisher</code> and in receive method I extracted <code>cacheIfPossible()</code> method:</p>

<p>{% codeblock DataPublisher lang:scala <a href="https://github.com/Zuchos/akka-http-with-streams/blob/blogpost2/src/main/scala/pl/zuchos/example/actors/DataPublisher.scala">https://github.com/Zuchos/akka-http-with-streams/blob/blogpost2/src/main/scala/pl/zuchos/example/actors/DataPublisher.scala</a> %}</p>

<p>  override def receive: Actor.Receive = {
    case Publish(s) =>
      cacheIfPossible(s)
    case Request(cnt) =>
      publishIfNeeded()
    case Cancel => context.stop(self)
    case _ =>
  }</p>

<p>  private def cacheIfPossible(s: Data) {
    if (queue.length == bufferSize) {
      sender() ! Failure(new BufferOverflow)
    } else {
      queue.enqueue(s)
      sender() ! Success()
      publishIfNeeded()
    }
  }</p>

<p>{% endcodeblock %}</p>

<!--more-->


<p>So the main change here is that we are testing buffer size, if the buffer is full we respond with <code>Failure</code> to the Sender else we are adding data to the buffer and sending back <code>Success</code>. In <code>SimpleService</code> we are <strong>asking</strong> instead of <strong>talking</strong> to actor and then map the answer (<code>Try</code>) to the correct <code>HttpResponse</code>.</p>

<p>{% codeblock SimpleService lang:scala <a href="https://github.com/Zuchos/akka-http-with-streams/blob/blogpost2/src/main/scala/pl/zuchos/example/NaiveServer.scala">https://github.com/Zuchos/akka-http-with-streams/blob/blogpost2/src/main/scala/pl/zuchos/example/NaiveServer.scala</a> %}</p>

<pre><code>  path("data") {
    (post &amp; entity(as[String]) &amp; parameter('sender)) {
      (dataAsString, sender: String) =&gt;
        complete {
          val publisherResponse: Future[Any] = dataPublisherRef ? Publish(Data(sender, dataAsString))
          publisherResponse.map {
            case Success(_) =&gt; HttpResponse(StatusCodes.OK, entity = "Data received")
            case Failure(_: BufferOverflow) =&gt; HttpResponse(StatusCodes.ServiceUnavailable, entity = "Try again later...")
            case _ =&gt;
              HttpResponse(StatusCodes.InternalServerError, entity = "Something gone terribly wrong...")
          }
        }
    }
</code></pre>

<p>{% endcodeblock %}</p>

<p>That solution is better than the previous one and for my case it fits better than using <code>Source.actorRef</code>. It&rsquo;s also not fully reactive (we don&rsquo;t forward back pressure to the client). Besides that we trust that the declared buffer will fit in memory, which is not too smart for the production. I tried to use <a href="https://github.com/akka/akka/blob/release-2.3-dev/akka-stream/src/main/scala/akka/stream/impl/FixedSizeBuffer.scala"><code>FixedSizeBuffer.scala</code></a> but it&rsquo;s a private class.</p>

<p>Besides those cons my solution still exposes simple REST API and process data asynchronously. You may use <strong>akka-streams</strong> advantage to define compliated processing graph easily.</p>

<p>This time I wrote better <a href="https://github.com/Zuchos/akka-http-with-streams/blob/blogpost2/src/test/scala/pl/zuchos/example/SimpleServiceSpec.scala">tests</a> and added notion of DI. Now we don&rsquo;t need to define data processing definition inside the SimpleService, we may pass it along. It makes <code>SimpleService</code> more simple and generic, if that would be parametrized with generic type (as well as <code>DataPublisher</code> actor) it might be used for different kinds of incoming data. I learned about one more thing during writing tests: initial demand. Even when buffer was set to 2 elements (and the <a href="https://github.com/Zuchos/akka-http-with-streams/blob/blogpost2/src/test/scala/pl/zuchos/example/LazyDataSubscriber.scala"><code>LazyDataSubscriber</code></a> wasn&rsquo;t demanding anything), I needed to send at least one more request to overflow buffer. That&rsquo;s because of the initial demand (which was draining buffer). The inital demand is defined at <code>Sink</code> level: <code>Sink(dataSubscriber).withAttributes(OperationAttributes.inputBuffer(1, 1))</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to write a publisher for akka-streams?]]></title>
    <link href="http://zuchos.github.io/blog/2015/05/23/how-to-write-a-subscriber-for-akka-streams/"/>
    <updated>2015-05-23T00:05:10+02:00</updated>
    <id>http://zuchos.github.io/blog/2015/05/23/how-to-write-a-subscriber-for-akka-streams</id>
    <content type="html"><![CDATA[<p>Recently I started using <em>akka-http</em> and what I was trying to achieve was to receive data from request, send response that the data were received successfully and then process it asynchronously. The other requirement was that the processing flow could be complicated in the future and some parts of it could be faster than other, so I decided to use <em>akka-streams</em> for that. I started with empty <em>akka-http</em> service:</p>

<p>{% codeblock SimpleService lang:scala <a href="https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala">https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala</a> %}</p>

<p>  trait SimpleService {</p>

<pre><code>implicit val system: ActorSystem
implicit def executor: ExecutionContextExecutor
implicit val materializer: FlowMaterializer

val routes = {
  path("hello") {
    get {
      complete("Hello World!")
    }
  }
}
</code></pre>

<p>  }</p>

<p>  object NaiveGsServer extends App with SimpleService {</p>

<pre><code>override implicit val system = ActorSystem()
override implicit val executor = system.dispatcher
override implicit val materializer = ActorFlowMaterializer()

val config = ConfigFactory.load()

Http().bindAndHandle(routes, config.getString("http.host"), config.getInt("http.port"))
</code></pre>

<p>  }
{% endcodeblock %}</p>

<!--more-->


<p>Now we want to add new route that will accept data from sender. For this purpose we are going to add it to the routing definition.</p>

<p>{% codeblock lang:scala routes <a href="https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala">https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala</a> %}
  val routes = {
    path(&ldquo;hello&rdquo;) {
      get {
        complete(&ldquo;Hello World!&rdquo;)
      }
    } ~
    path(&ldquo;data&rdquo;) {
      (post &amp; entity(as[String]) &amp; parameter(&lsquo;sender.as[String])) {
        (dataAsString, sender: String) =>
          complete {
            HttpResponse(StatusCodes.OK, entity = &ldquo;Data received&rdquo;)
          }
      }
    }
  }
{% endcodeblock %}</p>

<p>What is now missing is the Publisher that will publish data that came from http request into the akka-stream. To do that we need to define <code>DataPublisher</code>. It will be an implementation of <code>ActorPublisher</code> trait. It will be receiving data and then it will be publishing those to the next element in the flow.</p>

<p>{% codeblock lang:scala DataPublisher <a href="https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/actors/FramePublisher.scala">https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/actors/FramePublisher.scala</a> %}
  case class Data(sender: Option[String], body: String)</p>

<p>  class DataPublisher extends ActorPublisher[Data] {
    var queue: mutable.Queue[Data] = mutable.Queue()</p>

<pre><code>override def receive: Actor.Receive = {
    case Publish(s) =&gt; queue.enqueue(s)
    publishIfNeeded()
  case Request(cnt) =&gt;
    publishIfNeeded()
  case Cancel =&gt; context.stop(self)
  case _ =&gt;
}

def publishIfNeeded() = {
  while (queue.nonEmpty &amp;&amp; isActive &amp;&amp; totalDemand &gt; 0) {
    onNext(queue.dequeue())
  }
}
</code></pre>

<p>  }</p>

<p>  object DataPublisher {
    case class Publish(data: Data)
  }
{% endcodeblock %}</p>

<p>As you may see, the main method is <code>receive()</code> which is responsible for accepting the incoming data and responding on demand on data that is coming from subscribers.
The last thing is to define the processing flow.</p>

<p>{% codeblock lang:scala flow definition <a href="https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala">https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala</a> %}
  val dataPublisherRef = system.actorOf(Props[DataPublisher])
  val dataPublisher = ActorPublisher<a href="dataPublisherRef">Data</a></p>

<p>  Source(dataPublisher)
    .runForeach(
      (x: Data) =>
        println(s"Data from ${x.sender} are being processed: ${x.body}&ldquo;)
    )
    .onComplete(_ => system.shutdown())
{% endcodeblock %}  <br/>
and then publish the incoming data:</p>

<p>{% codeblock lang:scala publishing <a href="https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala">https://github.com/Zuchos/akka-http-with-streams/tree/blogpost1/src/main/scala/pl/zuchos/example/NaiveGsServer.scala</a> %}
  path(&ldquo;data&rdquo;) {
    (post &amp; entity(as[String]) &amp; parameter(&lsquo;sender.as[String])) {
    (dataAsString, sender: String) =>
        complete {
          dataPublisherRef ! Publish(Data(sender, dataAsString))
          HttpResponse(StatusCodes.OK, entity = &ldquo;Data received&rdquo;)
        }
    }
{% endcodeblock %}</p>

<p>Now your application is ready to process incoming data with akka-streams. You may find complete example on <a href="https://github.com/Zuchos/akka-http-with-steams">github</a></p>
]]></content>
  </entry>
  
</feed>
